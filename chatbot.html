<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Optimizing Large Language Models (LLM) to answer South African Food Recipes Questions</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      font-size: 14px;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #1a1a1a;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    ul, li {
      margin-left: 20px;
    }
    pre, code {
      background-color: #eee;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 13px;
    }
    .section {
      margin-bottom: 40px;
    }
  </style>
</head>
<body>
  <h1>Optimizing Large Language Models (LLM) to respond to inquiries about South African cuisine</h1>

  <p>
  Within this project, I developed a chatbot—an increasingly valuable tool in today's digital landscape. 
  Chatbots are rapidly becoming essential for businesses, serving various functions such as customer service, sales support, and interactive engagement.
    One of the key challenges in deploying these chatbots is ensuring they are effectively customized to answer specific questions.
    This requires tailoring responses to align with user needs and expectations, making the chatbot more responsive and intelligent. 
    By addressing these challenges, businesses can significantly enhance user engagement and improving customer satisfaction.

  </p>

  <div class="section">
    <h2>Optimizing GPT-2-medium to answer South African Food </h2>
    <p>
      In this project, I focused on optimizing a simple GPT-2-medium model. 
      GPT-2 was chosen because it does not require an external API, is lightweight enough to run on low-end hardware.
      In this project, my goal was to fine-tune GPT-2 to intelligently answer questions about South African recipes, bringing a deeper understanding of the country's rich culinary traditions.
      South African cuisine is a merge of diverse cultural influences, ranging from indigenous flavors to Cape Malay, Dutch, and Indian inspirations. 
      By adapting GPT-2, I aimed to create a chatbot capable of providing insights into traditional famous dishes such as Bobotie, Bunny Chow, Koeksisters, and Biltong.  
    </p>
  </div>

  <div class="section">
    <h2>Brief introduction to Large Language Model?</h2>
    <p>
      At its core, a Large Language Model (LLM) is a neural network trained to predict the next word in a sentence. It processes billions of lines of text to learn patterns, syntax, and meaning. The process—known as tokenization and embedding—breaks text into manageable pieces, with each token receiving a numerical identity based on its context.
      The evolution of models leading to language models:
    </p>
    <ul>
      <li>
        <i>Sequence-to-Sequence (Seq2Seq) Learning:</i> Introduced by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, this paper introduces an encoder-decoder architecture where the encoder compresses the input into a fixed-size vector and the decoder generates the output.
      </li>
      <li>
        <i>Attention Mechanisms:</i> Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio introduced the concept of  attention in “Neural Machine Translation by Jointly Learning to Align and Translate.” This allowed the decoder to focus dynamically on relevant sections of the input rather than compressing all information into a single vector.
      </li>
      <li>
        <i>Transformers:</i> Building on these ideas, Ashish Vaswani et al. proposed the transformer model in “Attention is All You Need.” Transformers use self-attention mechanisms to capture relationships between all elements of the input simultaneously.
      </li>
    </ul>
    <p>
      Modern LLMs are built using transformer architecture. This framework, based on self-attention, enables models to focus on relevant parts of a sentence regardless of word order, resulting in coherent and contextually informed responses.
    </p>
    <p>
      The core architecture of a transformer can be explained in the following steps:
    </p>
    <ul>
      <li><strong>Input/Output Embedding:</strong> Conversion of words into numerical vectors.</li>
      <li><strong>Positional Embedding:</strong> Encoding the position of tokens in the sequence.</li>
      <li><strong>Encoder and Decoder:</strong> Processing input and generating output through a two-part system.</li>
      <li><strong>Multi-Head Attention:</strong> Capturing different aspects of relationships between tokens.</li>
      <li><strong>Multi-Masked Attention:</strong> Allowing the model to consider only the relevant context during training.</li>
    </ul>
    <p>
      To ensure high accuracy, LLMs are trained on vast amounts of text data spanning billions of pages. This exhaustive training enables them to grasp deep semantic and conceptual relationships, allowing for the generation of coherent and contextually relevant text.
    </p>

    <p>
      Despite their impressive capabilities, LLMs face several challenges:
    </p>
    <ul>
      <li><i>Overfitting & Memorization:</i> Some models may simply recall patterns without truly understanding concepts.</li>
      <li><i>Bias in Training Data:</i> Since LLMs learn from pre-existing datasets, they can inadvertently adopt undesirable biases.</li>
      <li><i>Data Dependency:</i> The model’s knowledge is limited to its training data, restricting its adaptability to new or unseen information.</li>
    </ul>
    
  </div>

  <div class="section">
    <h2>Optimization Techniques</h2>
    <p>
      To enhance the performance of the GPT-2 model to answer questions on South African food Recipes. I focused on several key optimization techniques:
    
   
        <i>Fine-Tuning:</i> I fine-tuned GPT-2 using specific database content to enhance its accuracy in responding to questions about traditional South African cuisine. 
        Fine-tuning involves refining a pre-trained model for specialized domains using a relatively small yet relevant dataset, allowing it to develop a deeper understanding of specific topics. 
        One significant challenge in this process was the absence of a dedicated dataset exclusively focused on South African recipes. 
        To overcome this limitation, I utilized a large-scale, generic recipe dataset from Kaggle, which provided a broad foundation for training.
        However, using datsets which are often static and fixed this can limit the model’s ability to adapt to evolving trends, updated facts, or newexisting knowledge.
        <i>Retrieval-Augmented Generation (RAG):</i> RAG allows the model to incorporate updated information without requiring complete retraining. 
        By building a vector database (using systems like Chroma DB) from PDF documents, the model retrieves relevant context and integrates it into its responses. 
        This approach is especially useful when data changes frequently and retraining is impractical. such as finance, healthcare, or specialized knowledge domains, making periodic retraining impractical.
        In this project, I implemented RAG by using the "South African Traditional Recipes 50 cookbook" by GJ van Niekerk, enabling the model to provide customized responses tailored to South African cuisine. 
        By structuring the cookbook content as embeddings in the vector database, the chatbot can retrieve rich culinary details—including traditional recipes, ingredient insights, and preparation techniques—ensuring culturally accurate responses
        <i>Reinforcement Learning from Human Feedback (RLHF):</i> This approach focuses on Reinforcement Learning from Human Feedback (RLHF)—a method that significantly enhances model performance by using direct user feedback.
        The process involves collecting human evaluations of model responses, training a reward model based on this feedback, and then fine-tuning the original  model using reinforcement learning.
      </p>
    
    <p>
       <i>When to Choose RAG Over Fine-Tuning</i>  If your data is highly dynamic or if you are limited by computational resources, RAG is a more practical choice. It does not require complete model retraining; instead, it relies on a robust retrieval system and vector database to fetch updated content. By chunking and embedding documents and then integrating that context into GPT-2 via prompt engineering, you can maintain both accuracy and timeliness.
    </p>
  </div>

  <div class="section">
    <h2>Final THoughts</h2>
    <p> 
        One of the biggest challenges I face in optimizing a Large Language Model (LLM) for South African traditional food is the lack of comprehensive datasets. 
      Finding well-structured, authentic sources on traditional recipes has proven difficult, which limits the depth and accuracy of my responses. 
      To address this, I need to expand my dataset by incorporating additional PDF documents, particularly well-known South African cookbooks, 
      as they offer reliable insights into culinary traditions and preparation methods.

Formatting responses in a more structured way is another improvement. By extracting and presenting information in summaries or bullet-point reports, 
      response can be clearer and easier to understand. This ensures users can quickly grasp key details without having to sift through long paragraphs.

Speed is also an area that  needs optimization. The response generation time can sometimes be slow, and improving computational efficiency, 
      refining model architecture, and fine-tuning parameters will help provide quicker replies. 
      Another issue is truncated outputs, which may be due to training on a medium-sized LLM model with limited parameters. 



    </p>

   
  </div>

  <div class="section">
    <h2>Building a Simple UI with Gradio</h2>
    <p>
      To make this project accessible, I built a clean and intuitive front-end using Gradio. With this interface, users can ask questions, receive context-aware answers, and even view the documents that contributed to each response.
    </p>
  </div>

  <div class="section">
    <h2>Reference</h2>
    <p>
      For more information on large language models, visit
      <a href="https://www.ibm.com/think/topics/large-language-models" target="_blank">IBM Think: Large Language Models</a>.
    </p>
  </div>
</body>
</html>
