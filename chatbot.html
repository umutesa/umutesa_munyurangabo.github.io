<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Optimizing Large Language Models (LLM) to answer South African Food Recipes Questions</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      font-size: 14px;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #1a1a1a;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    ul, li {
      margin-left: 20px;
    }
    pre, code {
      background-color: #eee;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 13px;
    }
    .section {
      margin-bottom: 40px;
    }
  </style>
</head>
<body>
  <h1>Optimizing Large Language Models (LLM) to respond to inquiries about South African cuisine</h1>

  <p>
  Within this project, I developed a chatbot—an increasingly valuable tool in today's digital landscape. 
  Chatbots are rapidly becoming essential for businesses, serving various functions such as customer service, sales support, and interactive engagement.
    One of the key challenges in deploying these chatbots is ensuring they are effectively customized to answer specific questions.
    This requires tailoring responses to align with user needs and expectations, making the chatbot more responsive and intelligent. 
    By addressing these challenges, businesses can significantly enhance user engagement and improving customer satisfaction.

  </p>

  <div class="section">
    <h2>Optimizing GPT-2-medium to answer South African Food </h2>
    <p>
      In this project, I focused on optimizing a simple GPT-2-medium model. 
      GPT-2 was chosen because it does not require an external API, is lightweight enough to run on low-end hardware.
      In this project, my goal was to fine-tune GPT-2 to intelligently answer questions about South African recipes, bringing a deeper understanding of the country's rich culinary traditions.
      South African cuisine is a merge of diverse cultural influences, ranging from indigenous flavors to Cape Malay, Dutch, and Indian inspirations. 
      By adapting GPT-2, I aimed to create a chatbot capable of providing insights into traditional famous dishes such as Bobotie, Bunny Chow, Koeksisters, and Biltong.  
    </p>
  </div>

  <div class="section">
    <h2>Brief introduction to Large Language Model?</h2>
    <p>
      At its core, a Large Language Model (LLM) is a neural network trained to predict the next word in a sentence. It processes billions of lines of text to learn patterns, syntax, and meaning. The process—known as tokenization and embedding—breaks text into manageable pieces, with each token receiving a numerical identity based on its context.
      The evolution of models leading to language models:
    </p>
    <ul>
      <li>
        <i>Sequence-to-Sequence (Seq2Seq) Learning:</i> Introduced by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, this paper introduces an encoder-decoder architecture where the encoder compresses the input into a fixed-size vector and the decoder generates the output.
      </li>
      <li>
        <i>Attention Mechanisms:</i> Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio introduced the concept of  attention in “Neural Machine Translation by Jointly Learning to Align and Translate.” This allowed the decoder to focus dynamically on relevant sections of the input rather than compressing all information into a single vector.
      </li>
      <li>
        <i>Transformers:</i> Building on these ideas, Ashish Vaswani et al. proposed the transformer model in “Attention is All You Need.” Transformers use self-attention mechanisms to capture relationships between all elements of the input simultaneously.
      </li>
    </ul>
    <p>
      Modern LLMs are built using transformer architecture. This framework, based on self-attention, enables models to focus on relevant parts of a sentence regardless of word order, resulting in coherent and contextually informed responses.
    </p>
    <p>
      The core architecture of a transformer can be explained in the following steps:
    </p>
    <ul>
      <li><strong>Input/Output Embedding:</strong> Conversion of words into numerical vectors.</li>
      <li><strong>Positional Embedding:</strong> Encoding the position of tokens in the sequence.</li>
      <li><strong>Encoder and Decoder:</strong> Processing input and generating output through a two-part system.</li>
      <li><strong>Multi-Head Attention:</strong> Capturing different aspects of relationships between tokens.</li>
      <li><strong>Multi-Masked Attention:</strong> Allowing the model to consider only the relevant context during training.</li>
    </ul>
    <p>
      To ensure high accuracy, LLMs are trained on vast amounts of text data spanning billions of pages. This exhaustive training enables them to grasp deep semantic and conceptual relationships, allowing for the generation of coherent and contextually relevant text.
    </p>

    <p>
      Despite their impressive capabilities, LLMs face several challenges:
    </p>
    <ul>
      <li><i>Overfitting & Memorization:</i> Some models may simply recall patterns without truly understanding concepts.</li>
      <li><i>Bias in Training Data:</i> Since LLMs learn from pre-existing datasets, they can inadvertently adopt undesirable biases.</li>
      <li><i>Data Dependency:</i> The model’s knowledge is limited to its training data, restricting its adaptability to new or unseen information.</li>
    </ul>
    
  </div>

  <div class="section">
    <h2>Optimization Techniques</h2>
    <p>
      To enhance the performance of the GPT-2 model to answer questions on South African food Recipes. I focused on three optimization methods:
    
   
        <i> a) Fine-Tuning:</i> I fine-tuned GPT-2 using specific database content to enhance its accuracy in responding to questions about traditional South African cuisine. 
        Fine-tuning involves refining a pre-trained model for specialized domains using a relatively small yet relevant dataset, allowing it to develop a deeper understanding of specific topics. 
        One significant challenge in this process was the absence of a dedicated dataset exclusively focused on South African recipes. 
        To overcome this limitation, I utilized a large-scale, generic recipe dataset from Kaggle, which provided a broad foundation for training.
        However, using datsets which are often static and fixed this can limit the model’s ability to adapt to evolving trends, updated facts, or newexisting knowledge.
        <i>b) Retrieval-Augmented Generation (RAG):</i> RAG allows the model to incorporate updated information without requiring complete retraining. 
        By building a vector database (using systems like Chroma DB) from PDF documents, the model retrieves relevant context and integrates it into its responses. 
        This approach is especially useful when data changes frequently and retraining is impractical. such as finance, healthcare, or specialized knowledge domains, making periodic retraining impractical.
        In this project, I implemented RAG by using the "South African Traditional Recipes 50 cookbook" by GJ van Niekerk, enabling the model to provide customized responses tailored to South African cuisine. 
        By structuring the cookbook content as embeddings in the vector database, the chatbot can retrieve rich culinary details—including traditional recipes, ingredient insights, and preparation techniques—ensuring culturally accurate responses.
        <i>c) Reinforcement Learning from Human Feedback (RLHF):</i> This approach focuses on Reinforcement Learning from Human Feedback (RLHF)—a method that significantly enhances model performance by using direct user feedback.
        The process involves collecting human evaluations of model responses, training a reward model based on this feedback, and then fine-tuning the original  model using reinforcement learning.
      </p>
    
    <p>
       <i>When to Choose RAG Over Fine-Tuning</i>  If your data is highly dynamic or if you are limited by computational resources, RAG is a more practical choice. It does not require complete model retraining; instead, it relies on a robust retrieval system and vector database to fetch updated content. By chunking and embedding documents and then integrating that context into GPT-2 via prompt engineering, you can maintain both accuracy and timeliness.
    </p>
  </div>

  <div class="section">
    <h2>Final Thoughts</h2>

    <script>

      document.write("
        // Example of adding dynamic output
       Initializing CPU-optimized Chatbot...
Loading dataset from: /root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/recipes.json
Loading PDF from: https://yourknow.com/uploads/books/5dd0f9604c895.pdf
Loading existing vector database from: hybrid_vectors
Loading language model: gpt2-medium

Device set to use cpu

Setting up language model pipeline...
Chatbot initialization complete!
Q: How to make milk tart?
Feedback saved to rlhf_feedback.json
A: 
        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!

  Question :    "How do you know if your buttercream will be too soft?" Answer :         You need to use 1/2 cup of melted fat. The more saturated fats are used, it's easier for them not melt into each other when they're mixed with water (like coconut oil). If there isn't enough melting point on top, then its hard to get all the oils from one piece together so just add as much extra cooking liquid like 2 tablespoons at first before adding any additional ingredients until everything melts evenly. It should look something like below after mixing up my mixture...
                          This was really easy because we had already made our own whipped cream which worked out great but still needed adjusting later - see how different things looked once blended? So here goes... First off let me say thank goodness i didn´t have to buy anything else since these were homemade! They came pre-made ready milled by hand using only 3 cups of flour, no baking powder etc.. And yes those biscuits did come very well packed inside :) But what makes their taste even better than others?? Well thanks again guys!! We'll definitely try making another batch soon!! Thank u!!! Reply Delete Comment Add comment 0 comments Read Comments » Leave a review | View previous reviews <<< Previous Review By Anonymous On Nov 17th 2017 @ 12 PM CST Rating:-5 Thanks everyone who participated today....I'm glad someone took time away from work tonight.....and got back early tomorrow morning......it seems people enjoyed themselves alot yesterday....thank yall!!!!!!!!! Posted by james_bobby on Sep 26nd 2016 @ 10 AM EST Rating:: 5 Love love LOVE IT!!!!!!!! SO GOOD TO SEE YOU ALL COMING BACK FOR MORE SHOWS AND FUNS WITH US...............THANKYOU THANK YOU EVERYONE WHO JOINED TODAY AT 9AM THIS MORNING..WE WILL BE WORKING ON A NEW SHOW IN THE NEXT MONNDAY......PLEASE CHECK OUR PAGE AGAINST EMAIL OR CALL IF WE ARE NOT UP BY THAT TIME.................IF THERE IS ANYTHING ELSE NEEDED PLEASE LET ME KNOW ASap................….thanks Again James B*******Posted Date Jul 27st 2015@ 11PM PSTRating 4 stars(outoffive)Thanks Everyone Who Participated Today In Our New Show Tonight At 8pm ET Join us For More Shows & Fun With Us........................Thankyou To All Of Those That Came Out Tomorrow Evening................For Your Support AswellAsToThoseThat Didn`t Come Back After Work Day Because Of Weather Or Other Reasons
--------------------------------------------------
")
    </script>

    
    <p> 

      The responses generated were informal, lacked proper structure, and were not thoroughly filtered. 
      Since most available datasets did not include a significant collection of South African recipes, 
      the primary source for generating content was a PDF cookbook. 
      However, this cookbook was not well-formatted, as it contained a mix of images, comments, and unstructured text. 
      Due to the simplicity of the language model built, the selection process was based on filtering data as best as possible to generate relevant responses. 
      Despite these efforts, the output still required refinement to ensure clarity, consistency, and improved structure.
A more better approach to data preprocessing is necessary. 
      This includes extracting and cleaning raw text from the PDF, eliminating unnecessary elements such as images and informal comments, and reorganizing the recipe content to present 
      well-structured instructions. Additionally, refining the filtering process can help ensure that only the most relevant and accurate information is selected. 

Furthermore, integrating multiple datasets specifically focused on South African cuisine would improve the model’s accuracy and relevance,
      providing a richer, more culturally representative culinary resource.
      By applying these improvements, the model can generate well-structured, high-quality recipe responses that align with user expectations. If further optimization techniques are needed, I’d be happy to explore them!
        One of the biggest challenges I face while optimizing a Large Language Model (LLM) for South African traditional food is the lack of comprehensive datasets. 
      Finding well-structured, authentic sources on South African traditional recipes has proven difficult, which limits the depth and accuracy of the responses. 
      To address this, I expand the LLM knowledge by incorporating additional PDF documents, particularly well-known South African cookbooks, 
      as they offer reliable insights into culinary traditions and preparation methods.

Formatting responses in a more structured format is another area that requires improvement. By extracting and presenting information in summaries or bullet-point reports, 
      response can be clearer and easier to understand. This ensures users can quickly grasp key details without having to read through long paragraphs, which areoften direct text from data sources.

Speed is also an area that needs optimization. The response generation time can  be slow, and improving computational efficiency by 
      refining model architecture, and fine-tuning parameters will help provide quicker replies. 
      Another issue is truncated outputs, which may be due to training on a medium-sized LLM model with limited parameters.

      



    </p>

   
  </div>

  <div class="section">
    <h2>Building a Simple UI with Gradio</h2>
    <p>
      To make this project accessible, I built a clean and very simple front-end using Gradio. With this interface, users can ask questions, 
      receive context-aware answers, and even view the documents that contributed to each response.
    </p>
  </div>

</body>
</html>
