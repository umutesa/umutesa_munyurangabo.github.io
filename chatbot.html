<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Optimizing Large Language Models (LLM) to answer South African Food Recipes Questions</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      font-size: 14px;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #1a1a1a;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    ul, li {
      margin-left: 20px;
    }
    pre, code {
      background-color: #eee;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 13px;
    }
    .section {
      margin-bottom: 40px;
    }
      .image-row {
    display: flex;
    justify-content: center;
    gap: 25px;
  }

  figure {
    text-align: center;
  }

  img {
    width: 500px;
    height: 400px;
    object-fit: fit;
    
  }

  figcaption {
    margin-top: 8px;
    font-style: italic;
    color: #555;
  }
  </style>
</head>
<body>
  <h1>Optimizing Large Language Models (LLM) to respond to inquiries about South African cuisine</h1>

  <p>
  Within this project, I developed a chatbot—an increasingly valuable tool in today's digital landscape. 
  Chatbots are rapidly becoming essential for businesses, serving various functions such as customer service, sales support, and interactive engagement.
    One of the key challenges in deploying these chatbots is to ensure that they are effectively customized to answer specific questions.
    This requires tailoring responses to align with user needs and expectations, making the chatbot more responsive and intelligent. 
   

  </p>

  <div class="section">
    <h2>Optimizing GPT-2-medium to answer South African Food </h2>
    <p>
      In this project, I focused on optimizing a simple GPT-2-medium model. 
      GPT-2 was chosen because it is lightweight enough to run on low-end hardware.
      In this project, my goal was to fine-tune GPT-2 to intelligently answer questions about South African recipes, bringing a deeper understanding of the country's rich culinary traditions.
      South African cuisine is a merge of diverse cultural influences, ranging from indigenous flavors to Cape Malay, Dutch, and Indian inspirations. 
      
    </p>
  </div>

  <div class="section">
    <h2>Brief introduction to Large Language Model?</h2>

    <img src="https://www.yadavsaurabh.com/content/images/size/w1000/2023/08/image-4.png" alt="Preprocessing pipeline leading to the transformer mode" width="600" height="400 style="object-fit:fit;object-position:center;">
    
    <p>
      At its core, a Large Language Model (LLM) is a neural network trained to predict the next word in a sentence. It processes billions of lines of text to learn patterns, syntax, and meaning. The process mainly tokenization and embedding; breaks text into manageable pieces, with each token receiving a numerical identity based on its context.
      The evolution of models leading to language models as shown in the image above:
    </p>
    <ul>
      <li>
        <i>Sequence-to-Sequence (Seq2Seq) Learning:</i> Introduced by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, this paper introduces an encoder-decoder architecture where the encoder compresses the input into a fixed-size vector and the decoder generates the output.
      </li>
      <li>
        <i>Attention Mechanisms:</i> Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio introduced the concept of  attention in “Neural Machine Translation by Jointly Learning to Align and Translate.” This allowed the decoder to focus dynamically on relevant sections of the input rather than compressing all information into a single vector.
      </li>
      <li>
        <i>Transformers:</i> Building on these ideas, Ashish Vaswani et al. proposed the transformer model in “Attention is All You Need.” Transformers use self-attention mechanisms to capture relationships between all elements of the input simultaneously.
      </li>
    </ul>
    <p>
      Modern LLMs are built using transformer architecture. This framework, based on self-attention, enables models to focus on relevant parts of a sentence regardless of word order, resulting in coherent and contextually informed responses.
    </p>
    <p>
      The core architecture of a transformer can be explained in the following steps:
      
      <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*Qb1YkJkJsh-OjoZg.png" alt="Preprocessing pipeline leading to the transformer mode" width="600" height="400" style="object-fit:fit;object-position:center;
>
    
    </p>
    <ul>
      <li><strong>Input/Output Embedding:</strong> Conversion of words into numerical vectors.</li>
      <li><strong>Positional Embedding:</strong> Encoding the position of tokens in the sequence.</li>
      <li><strong>Encoder and Decoder:</strong> Processing input and generating output through a two-part system.</li>
      <li><strong>Multi-Head Attention:</strong> Capturing different aspects of relationships between tokens.</li>
      <li><strong>Multi-Masked Attention:</strong> Allowing the model to consider only the relevant context during training.</li>
    </ul>
    <p>
      To ensure high accuracy, LLMs are trained on vast amounts of text data spanning billions of pages. This exhaustive training enables them to grasp deep semantic and conceptual relationships, allowing for the generation of coherent and contextually relevant text.
   
    </p>

    <p>
      Despite their impressive capabilities, LLMs face several challenges:
    </p>
    <ul>
      <li><i>Overfitting & Memorization:</i> Some models may simply recall patterns without truly understanding concepts.</li>
      <li><i>Bias in Training Data:</i> Since LLMs learn from pre-existing datasets, they can inadvertently adopt undesirable biases.</li>
      <li><i>Data Dependency:</i> The model’s knowledge is limited to its training data, restricting its adaptability to new or unseen information.</li>
    </ul>
    
  </div>

  <div class="section">
    <h2>Optimization Techniques</h2>
    <p>
      To enhance the performance of the GPT-2 model to answer questions on South African food Recipes. I focused on three optimization methods:

       <figure>
    <img src="https://dist.neo4j.com/wp-content/uploads/20230608064931/1LTfNOqZQB_M42KyvttjSyw.png" alt="Preprocessing pipeline">
    <figcaption>Fine Tuning</figcaption>
  </figure>

        <i> a) Fine-Tuning:</i> The model was fine-tuned using specific database content(i.e this is typically as datasets) to enhance its accuracy in responding to questions about traditional South African cuisine. 
        Fine-tuning involves refining a pre-trained model using a relatively small yet relevant dataset, allowing it to develop a deeper understanding of specific topics. 
        A particular challenge within this project was the absence of a dedicated dataset exclusively focused on South African recipes. 
        To overcome this limitation, I utilized a large-scale, generic recipe dataset from Kaggle, which provided a broad foundation for training.
        However, using datsets which are often static and fixed this can limit the model’s ability to adapt to evolving trends, updated facts, or newexisting knowledge.
        <br>
    
    <figure>
    <img src="https://bradsol.com/wp-content/uploads/2024/04/image-1-RAG-e1713883898899.jpg" alt="Transformer model">
    <figcaption>Retrieval-Augmented Generation (RAG)</figcaption>
  </figure>
      
        <i>b) Retrieval-Augmented Generation (RAG):</i> RAG allows the model to incorporate updated information without requiring complete retraining. 
        By building a vector database (using systems like Chroma DB) from PDF documents, the model retrieves relevant context and integrates it into its responses. 
        This approach is especially useful when data changes frequently and retraining is impractical. such as in finance, healthcare, or specialized knowledge domains, making periodic retraining impractical.
        In this project, I implemented RAG by using the *"South African Traditional Recipes 50 cookbook" by GJ van Niekerk*, enabling the model to provide customized responses tailored to South African cuisine. 
        By structuring the cookbook content as embeddings in the vector database, the chatbot can retrieve rich culinary details—including traditional recipes, ingredient insights, and preparation techniques—ensuring culturally accurate responses.
        <br>

    <figure>
    <img src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/04/19/arch.png" alt="Transformer model">
    <figcaption>Reinforcement Learning from Human Feedback (RLHF)</figcaption>
  </figure>
      
        <i>c) Reinforcement Learning from Human Feedback (RLHF):</i> This focuses on Reinforcement Learning from Human Feedback (RLHF). A method that enhances model performance by using direct user feedback.
        The process involves collecting human evaluations of model responses, training a reward model based on this feedback, and then fine-tuning the original  model using reinforcement learning.
      </p>
    
    <p>
       <i>When to Choose RAG Over Fine-Tuning</i>  If your data is highly dynamic or if you are limited by computational resources, RAG is a more practical choice. It does not require complete model retraining; instead, it relies on a robust retrieval system and vector database to fetch updated content. By chunking and embedding documents and then integrating that context into GPT-2 via prompt engineering, you can maintain both accuracy and timeliness.
    </p>
  </div>

  <div class="section">
    <h2>Final Thoughts</h2>

    <script>

      document.write("
        // Example of adding dynamic output
       Initializing CPU-optimized Chatbot...
Loading dataset from: /root/.cache/kagglehub/datasets/crispen5gar/recipes3k/versions/1/recipes.json
Loading PDF from: https://yourknow.com/uploads/books/5dd0f9604c895.pdf
Loading existing vector database from: hybrid_vectors
Loading language model: gpt2-medium

Device set to use cpu

Setting up language model pipeline...
Chatbot initialization complete!
Q: How to make milk tart?
Feedback saved to rlhf_feedback.json
A: 
        I am a Chatbot assistant that can answer questions about South African Food recipes and some general recipes!

  Question :    "How do you know if your buttercream will be too soft?" Answer :         You need to use 1/2 cup of melted fat. The more saturated fats are used, it's easier for them not melt into each other when they're mixed with water (like coconut oil). If there isn't enough melting point on top, then its hard to get all the oils from one piece together so just add as much extra cooking liquid like 2 tablespoons at first before adding any additional ingredients until everything melts evenly. It should look something like below after mixing up my mixture...
                          This was really easy because we had already made our own whipped cream which worked out great but still needed adjusting later - see how different things looked once blended? So here goes... First off let me say thank goodness i didn´t have to buy anything else since these were homemade! They came pre-made ready milled by hand using only 3 cups of flour, no baking powder etc.. And yes those biscuits did come very well packed inside :) But what makes their taste even better than others?? Well thanks again guys!! We'll definitely try making another batch soon!! Thank u!!! Reply Delete Comment Add comment 0 comments Read Comments » Leave a review | View previous reviews <<< Previous Review By Anonymous On Nov 17th 2017 @ 12 PM CST Rating:-5 Thanks everyone who participated today....I'm glad someone took time away from work tonight.....and got back early tomorrow morning......it seems people enjoyed themselves alot yesterday....thank yall!!!!!!!!! Posted by james_bobby on Sep 26nd 2016 @ 10 AM EST Rating:: 5 Love love LOVE IT!!!!!!!! SO GOOD TO SEE YOU ALL COMING BACK FOR MORE SHOWS AND FUNS WITH US...............THANKYOU THANK YOU EVERYONE WHO JOINED TODAY AT 9AM THIS MORNING..WE WILL BE WORKING ON A NEW SHOW IN THE NEXT MONNDAY......PLEASE CHECK OUR PAGE AGAINST EMAIL OR CALL IF WE ARE NOT UP BY THAT TIME.................IF THERE IS ANYTHING ELSE NEEDED PLEASE LET ME KNOW ASap................….thanks Again James B*******Posted Date Jul 27st 2015@ 11PM PSTRating 4 stars(outoffive)Thanks Everyone Who Participated Today In Our New Show Tonight At 8pm ET Join us For More Shows & Fun With Us........................Thankyou To All Of Those That Came Out Tomorrow Evening................For Your Support AswellAsToThoseThat Didn`t Come Back After Work Day Because Of Weather Or Other Reasons
--------------------------------------------------
")
    </script>

    
    <p>


Despite the limited availability of datasets on South African cuisine, I successfully built a foundation for 
generating authentic recipe content. Using a PDF cookbook as the primary source. Despite its challenges with mixed formatting, images, and 
      informal notes I was able to filter and structure the data into meaningful, usable outputs. This marked an important step toward creating a culturally relevant recipe resource.

<br>      
Through this process, I demonstrated the value of targeted data preprocessing. I extracted and cleaned raw text, removed 
irrelevant elements, and organized recipes into clearer formats. These efforts allowed the model to deliver more structured and consistent results, 
      showing strong potential for high-quality recipe generation.

 <br>     
Looking ahead, I see opportunities to strengthen the project even further. Integrating multiple datasets dedicated 
specifically to South African cuisine will expand the model’s cultural depth and ensure even more authentic responses.
      I also plan to enhance formatting by presenting recipes in concise, easy-to-follow structures—such as step-by-step instructions or bullet points—so users can quickly grasp the essential details.

<br>      
Performance optimization has also been a key focus. I have identified ways to improve response speed and completeness, 
including refining the model’s architecture and fine-tuning parameters. 
      These improvements will reduce response times and minimize truncated outputs, ensuring a smoother user experience.

      



    </p>

   
  </div>


</body>
</html>
