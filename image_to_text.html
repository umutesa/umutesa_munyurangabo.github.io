<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Image-to-Text Synthesizer: Comparing Diffusion, CNNs, and GANs</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      font-size: 14px;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #1a1a1a;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    code {
      background-color: #eee;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 13px;
    }
    pre {
      background-color: #eee;
      padding: 10px;
      border-radius: 4px;
      overflow-x: auto;
    }
    .section {
      margin-bottom: 40px;
    }
  </style>
</head>
<body>
  <h1>Image-to-Text Synthesizer: Comparing Diffusion, CNNs, and GANs</h1>

  <p>Image-to-text conversion has become increasily popular, particular for answering questions about images and extracting visual data that can be transformative in fields like medicine and education. 
    This project delves into three fundamental deep learning models for image-text: Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs), and Diffusion Models (integrated with Vision Transformers and GPT-2). </p>

  <div class="section">
    <h2>Dataset Note</h2>
    <p>For this project, we used the well-known CIFAR-10 dataset, which comprises of 60,000 32x32 color images across 10 classes (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks). 
      Although the low resolution limits fine-grained detail extraction, CIFAR-10 was chosen for its simplicity , fast training and its ability to test model robustness under low hardware capabilities.</p>

    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/cifar10.jpg" alt="Confusion Matrix 1" width="600" height="600">

    
  </div>

  <div class="section">
    <h2>CNNs: The Backbone of Image Classification</h2>
    <p>Convolutional Neural Networks represent the earliest deep learning approach for image recognition.</p>
    
    <h3>How CNNs Work</h3>
    <ul>
      <li><strong>Convolution Layer:</strong> Filters (or kernels) slide over the image, computing dot products on local regions to create feature maps. 
        This process captures critical image features like edges, textures, and shapes..</li>
      <li><strong>Activation Layer:</strong> A non-linear activation function, such as ReLU, is applied to introduce non-linearity, allowing the network to model complex relationships.</li>
      <li><strong>Pooling Layer:</strong> Pooling (using max or average pooling) reduces spatial dimensions.</li>
    </ul>
    
    <p>After passing through several convolution and pooling layers, the resulting feature maps are flattened and fed into fully connected layers that perform the final classification. </p>

   
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/cnn_matrix.png" alt="Confusion Mtrix 1">
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/cnn-bar.png" alt="Performance Per Label 2" >
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/cnn_output.png" alt="Prediction Output" >
    
    
    <p>  
  ---

      Despite a small training size (approximately 10 epochs on CIFAR-10), the confusion matrix revealed strong image classification performance, averaging around 65% across all classes. 
      However, accuracy varied significantly: certain classes, such as airplanes (label 0) and cars (label 1), were recognized with high precision, while others, 
      including birds (label 2), cats (label 3) , and deer (label 4), showed comparatively lower performance, as illustrated in the per-label bar chart. </p>
    
  </div>

  <div class="section">
    <h2>GANs: Generative Adversarial Networks </h2>
    <p> 
      Generative Adversarial Networks (GANs) are a powerful deep learning framework for creating realistic images by utilizing two competing neural networks: a generator and a discriminator.
      The generator starts by producing images from random noise, while the discriminator assesses whether an image is real (from actual data) or fake (created by the generator).
      As training progresses, the generator refines its ability to mimic real images, while the discriminator continuously improves at distinguishing real from fake. 
    </p>
    
    <h3>How GANs Work</h3>
    <p>GANs operate with two distinct but interlinked neural networks:</p>
    <ul>
      <li><strong>Generator:</strong> Starting from a random noise vector, the generator uses transposed convolutions, batch normalization, and activations to create synthetic images designed to resemble real ones.</li>
      <li><strong>Discriminator:</strong> Structured similarly to a CNN, the discriminator evaluates both the real images from the dataset and the synthetic images, distinguishing between the two.</li>
    </ul>
    
    <p>Training is performed as a minimax game: the discriminator improves its ability to differentiate real from fake images, while the generator simultaneously learns to produce images that can fool the discriminator. 
    </p>

    <div style="display:flex;justify-content:space-between;flex-wrap:nowrap;"><img style="width:30%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan1.png" alt="Output Performance 1"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan2.png" alt="Output Performance 2"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan4.png" alt="Output Performance 3"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan3.png" alt="Output Performance 4"></div>
    
    
    <p>
        Generative Adversarial Networks (GANs) are highly effective at producing realistic images from noisy inputs, 
         but achieving strong results can be quite challenging. A major reason for this is their reliance on large , diverse training datasets ; 
      
         while carefully balancing the generator and discriminator losses.

      As illustrated in the provided images, training these particular GANs was especially difficult, primarily due to the small size of the training set.
      
      Additionally, GANs are designed to work with continuous data (like images) where gradients can be smoothly propagated.  Text generation consists of discrete tokens.
      
      This discrete nature makes it much harder for
      gradients to pass effectively between the generator and discriminator,
      
    
    </p>
  </div>

  <div class="section">
    <h2>Diffusion Models</h2>
    <p>
        Diffusion models, originally designed for generating high-quality images, have demonstrated significant promise in the field of image-to-text conversion. 
      Their core mechanism involves gradually adding random noise to an image. 

For image-to-text applications, diffusion models can be adapted to extract textual information from visual data, such as identifying and recognizing characters in noisy or low-resolution images. 
      
    </p>
    
    <h3>Vision Transformers (ViTs) and GPT-2 Integration</h3>
    <p>In our application, we combined a Vision Transformer (ViT) with GPT-2 to create an advanced image-to-text synthesizer:</p>
    <ul>
      <li><strong>Vision Transformer (ViT):</strong> The ViT partitions an image into patches, treating them as tokens and processing them with a 
        transformer architecture to capture global context.</li>
      <li><strong>GPT-2:</strong> A pre-trained language model that, when provided with features extracted by ViT, generates detailed and contextually rich captions.
        </li>
    </ul>

    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/diffusion.png" alt="Output Performanace">
    
    <p> This diffusion-based model generates descriptions that extend far beyond simple labels, 
      incorporating details such as background context and thereby providing a richer understanding of the image. </p>
  </div>

  <div class="section">
    <h2>Final thoughts</h2>
    <p>
      
      Convolutional Neural Networks (CNNs) are highly effective for image classification, delivering good results with efficient training. 
      CNNs are excellent at extracting local features and performing image classification, even with low-resolution images as shown within this project. 
      On the other hand, Generative Adversarial Networks (GANs)  are a creative approach to image reasoning by generating realistic images. They are ideal for handling noisy inputs,
      but require powerful tuning and 
      training and are not well-suited for image-to-text synthesis. 
        A significant challenge in GAN training lies in achieving the optimal balance between the generator and discriminator to minimize overall loss,  
  A simple classifier was employed to classify images into predefined labels by generating outputs conditioned on specific image labels; however, performance was suboptimal without sufficient training.
      The Diffusion Model (ViT + GPT-2) deliver the best context-rich textual descriptions, which are even more descriptive than the datasets labels.
      
     A key recommendation is to integrate traditional CNN or GAN models  with large language models (LLM) like GPT. 
      This hybrid approach may improve the efficiency of CNNs and GANs, and their contextual richness of image reasoning and understanding.
  
  </p>
  </div>
  
</body>
</html>
