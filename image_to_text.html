<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Image-to-Text Synthesizer: Comparing Diffusion, CNNs, and GANs</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #333;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      font-size: 14px;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #1a1a1a;
    }
    a {
      color: #0056b3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    code {
      background-color: #eee;
      padding: 2px 4px;
      border-radius: 4px;
      font-size: 13px;
    }
    pre {
      background-color: #eee;
      padding: 10px;
      border-radius: 4px;
      overflow-x: auto;
    }
    .section {
      margin-bottom: 40px;
    }
  </style>
</head>
<body>
  <h1>Image-to-Text Synthesizer: Comparing Diffusion, CNNs, and GANs</h1>

  <p>Image-to-text conversion has become increasily popular—for answering questions about images and extracting specific data that can be transformative in fields like medicine and education. 
    This project delves into three fundamental deep learning models for image-text: Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs), and Diffusion Models (integrated with Vision Transformers and GPT-2). </p>

  <div class="section">
    <h2>Dataset Note</h2>
    <p>For this project, we used the well-known CIFAR-10 dataset, which comprises 60,000 32x32 color images across 10 classes (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks). 
      Although the low resolution limits fine-grained detail extraction, CIFAR-10 was chosen for its simplicity , fast training and its ability to test model robustness under low hardware capabilities.</p>

    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/cifar10.jpg" alt="Confusion Matrix 1" width="600" height="600">

    
  </div>

  <div class="section">
    <h2>CNNs: The Backbone of Image Classification</h2>
    <p>Convolutional Neural Networks represent the earliest deep learning approach for image recognition.</p>
    
    <h3>How CNNs Work</h3>
    <ul>
      <li><strong>Convolution Layer:</strong> Filters (or kernels) slide over the image, computing dot products on local regions to create feature maps. This process captures essential features like edges, textures, and shapes, while weight sharing reduces the number of trainable parameters.</li>
      <li><strong>Activation Layer:</strong> A non-linear activation function, such as ReLU, is applied to introduce non-linearity, allowing the network to model complex relationships.</li>
      <li><strong>Pooling Layer:</strong> Pooling (using max or average pooling) reduces spatial dimensions, decreases computational complexity, and provides translation invariance.</li>
    </ul>
    
    <p>After passing through several convolution and pooling layers, the resulting feature maps are flattened and fed into fully connected layers that perform the final classification or feature extraction. </p>

   
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/cnn_matrix.png" alt="Confusion Mtrix 1">
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/cnn-bar.png" alt="Performance Per Label 2" >
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/cnn_output.png" alt="Prediction Output" >
    
    
    <p>  
      Even with a low training size (I trained around 10 epochs on CIFAR-10), the confusion matrix showed high performance in image classification, i.e Approximate 65% on average for all classes. 
      However certain classes (e.g., airplanes and cars) were accurately recognized, while others (like birds, cats, and deer) lagged behind as shown in the bar chart per label performance. </p>
    
  </div>

  <div class="section">
    <h2>GANs: Generative Adversarial Networks </h2>
    <p> 
      Generative Adversarial Networks (GANs) are a powerful deep learning framework for creating realistic images by utilizing two competing neural networks: a generator and a discriminator.
      The generator starts by producing images from random noise, while the discriminator assesses whether an image is real (from actual data) or fake (created by the generator).
      As training progresses, the generator refines its ability to mimic real images, while the discriminator continuously improves at distinguishing real from fake. 
    </p>
    
    <h3>How GANs Work</h3>
    <p>GANs operate with two distinct but interlinked neural networks:</p>
    <ul>
      <li><strong>Generator:</strong> Starting from a random noise vector, the generator uses transposed convolutions, batch normalization, and activations to create synthetic images designed to resemble real ones.</li>
      <li><strong>Discriminator:</strong> Structured similarly to a CNN, the discriminator evaluates both the real images from the dataset and the synthetic images, distinguishing between the two.</li>
    </ul>
    
    <p>Training is performed as a minimax game: the discriminator improves its ability to differentiate real from fake images, while the generator simultaneously learns to produce images that can fool the discriminator. 
    </p>

    <div style="display:flex;justify-content:space-between;flex-wrap:nowrap;"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan1.png" alt="Output Performance 1"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan2.png" alt="Output Performance 2"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan3.png" alt="Output Performance 3"><img style="width:25%;height:auto;" src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan4.png" alt="Output Performance 4"></div>
    
    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/gan_output.png" alt="Output Performanace">
    
    <p>GANs excel in generating realistic images from noisy data , though their training can be challenging due to issues like large training samples and the need for a delicate balance between the generator and discriminator.
    Training GANs proved to be particularly challenging. While they are generally expected to surpass CNNs in performance, this particular instance the GAN underperform due to limited training sing limited hardware.
    
    </p>
  </div>

  <div class="section">
    <h2>Diffusion Models</h2>
    <p>Diffusion models, often famed for creating visually stunning AI-generated art, have now expanded their utility into text extraction and recognition. By gradually adding noise to an image and learning to reverse this process, they effectively reconstruct missing or blurry details—enabling richer textual synthesis.</p>
    
    <h3>Vision Transformers (ViTs) and GPT-2 Integration</h3>
    <p>In our application, we combined a Vision Transformer (ViT) with GPT-2 to create an advanced image-to-text synthesizer:</p>
    <ul>
      <li><strong>Vision Transformer (ViT):</strong> The ViT partitions an image into patches, treating them as tokens and processing them with a transformer architecture to capture global context—a stark contrast to the primarily local feature extraction of CNNs.</li>
      <li><strong>GPT-2:</strong> A pre-trained language model that, when provided with features extracted by ViT, generates detailed and contextually rich captions. This synergy produces descriptions that go well beyond simple labels, including details like the background context and object actions.</li>
    </ul>

    <img src="https://raw.githubusercontent.com/umutesa/umutesa_munyurangabo.github.io/main/image/image2text/diffusion.png" alt="Output Performanace">
    
    <p>This diffusion-based approach not only provided superior accuracy on CIFAR-10, but also generated comprehensive descriptions that enriched the understanding of the scene—a significant advantage in fields like medical diagnostics and educational content creation.</p>
  </div>

  <div class="section">
    <h2>Final thoughts</h2>
    <p>Each model offers unique strengths in the realm of image-to-text synthesis:
      
      Convolutional Neural Networks (CNNs) are highly effective for image classification, delivering reliable results with efficient training. CNNs are Excellent at efficiently extracting local features and performing image classification, though they may on low-resolution images. 
      On the other hand, Generative Adversarial Networks (GANs)  are a creative edge by generating realistic images and filling in missing text components. They are ideal for handling noisy inputs, but require powerful tuning and  training and are not well-suited for image-to-text synthesis and do not perform classification tasks. 
      One of the biggest challenges in training GANs is finding the right balance between the generator and discriminator to minimize overall loss, 
      which requires extensive training and careful parameter tuning. 
      To classify images into predefined labels, we employed a Conditional GAN (cGAN), leveraging its ability to generate outputs conditioned on specific descriptions.
      Diffusion Models (ViT + GPT-2) deliver the most holistic, context-rich textual descriptions
     A key recommendation is to explore integrating traditional CNN or GAN models  with large language models (LLM) like GPT. 
      Such hybrid models can improve the efficiency of CNNs and GANs, and their contextual richness of image understanding.
  
  </p>
  </div>
  
</body>
</html>
